{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一共有1326个句子，32个intent，\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Untitled1.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import logging\n",
    "import numpy\n",
    "import pickle\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import json\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import statsmodels.api as sm \n",
    "import random \n",
    "import jieba\n",
    "import Ipynb_importer\n",
    "import Untitled1\n",
    "from Untitled1 import preprocess_data, generate_id, load_pretrained_word2vec\n",
    "from Untitled1 import generate_intent2text_intent_texts_len, generate_support_query\n",
    "from Untitled1 import sample_sq, dict_2_2array, Fewshot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_init_logging_handler() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ba4d95175caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0m_init_logging_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: _init_logging_handler() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "def _init_logging_handler(self):\n",
    "        current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "        stderr_handler = logging.StreamHandler()\n",
    "        file_handler = logging.FileHandler('./log/log_{}.txt'.format(current_time))\n",
    "        logging.basicConfig(handlers=[stderr_handler, file_handler])\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.INFO)\n",
    "_init_logging_handler()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gongan_train_10.17.json') as f:\n",
    "    load_json = json.load(f)\n",
    "    data = load_json['rasa_nlu_data']['common_examples']\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1,intentlist = preprocess_data(data)#把同义句低于5个的intent删除\n",
    "generate_id(data1)#生成word2id,id2word,intent2id,id2intent，jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转化成数字label\n",
    "with open('intent2id','rb') as f:\n",
    "    intent2id = pickle.load(f)\n",
    "with open('word2id','rb') as f:\n",
    "    word2id = pickle.load(f)   \n",
    "for item in data1:\n",
    "    item['intent'] = intent2id[item['intent']]\n",
    "    item['text'] = [word2id[w] for w in item['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "id_a = list(range(len(intentlist)))\n",
    "id_t = random.sample(id_a, int(0.75 * len(intentlist)))\n",
    "id_v = list(set(id_a)-set(id_t))\n",
    "intent_train = [intent2id[intentlist[i]] for i in id_t]\n",
    "intent_test = [intent2id[intentlist[i]] for i in id_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#做一个intent的同义句集合(intent_same记录同义句数量)，集合i位置代表intent_i的同义句有多少个，。筛选intent同义句大于15个的data条数，\n",
    "intent_same = [0 for i in range(355)]\n",
    "#print(intent_same)\n",
    "#train_data1筛选出intent同义句>15\n",
    "train_data = []\n",
    "test_data = []\n",
    "for item in data1:\n",
    "    if item['intent'] in intent_train:\n",
    "        train_data.append(item)\n",
    "    else:\n",
    "        test_data.append(item)\n",
    "print(len(train_data),len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查text的长度分布,截断长度\n",
    "def len_cut_20(data):\n",
    "    text_len = [0 for i in range(100)]\n",
    "    for item in data:\n",
    "        if(len(item['text']))<100:\n",
    "            text_len[len(item['text'])]+=1\n",
    "    max_len = 20\n",
    "    for item in data:\n",
    "        length = len(item['text'])\n",
    "        if length<20:\n",
    "            item['text'].extend([0] * (max_len - length))\n",
    "        else:\n",
    "            item['text'] = item['text'][:20]\n",
    "len_cut_20(train_data)\n",
    "len_cut_20(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建最终数据,生成传入模型的intent2text，intent_texts_len\n",
    "intent2text, intent_texts_len = generate_intent2text_intent_texts_len(train_data)\n",
    "t_intent2text,t_intent_texts_len = generate_intent2text_intent_texts_len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eposide_iter = int((28*27*26*25*24)/(5*4*3*2))\n",
    "way = 5\n",
    "shot = 10\n",
    "query = 20\n",
    "dynamic_iter = 3\n",
    "k = 100\n",
    "d_a = 64\n",
    "emb = 300\n",
    "hid=128\n",
    "lr =  0.0001\n",
    "PATH = './model_iter2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('renmin_emb_weights','rb') as f:\n",
    "    emb_weights = pickle.load(f)\n",
    "with open('word2id','rb') as f:\n",
    "    word2id = pickle.load(f)  #2889个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_classes = list(itertools.combinations(intent_train,5))\n",
    "print(len(train_sample_classes))\n",
    "print(eposide_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Fewshot(num_words=len(word2id),embed_size=emb,hidden_size=hid,emb_weight =emb_weights,\n",
    "                   way=way,shot=shot,query=query,dynamic_iter=dynamic_iter,k=k,d_a=d_a)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('./model/42500.pt'))\n",
    "model.cuda()\n",
    "optim = Adam(lr=lr, params=filter(lambda x: x.requires_grad, model.parameters()),weight_decay=5e-4)\n",
    "train_sample_classes = list(itertools.combinations(intent_train,5))\n",
    "test_sample_classes = list(itertools.combinations(intent_test,5))\n",
    "\n",
    "\n",
    "for training_iter in range(1):\n",
    "    for i in range(len(train_sample_classes)):       \n",
    "        sample_classes = train_sample_classes[i]\n",
    "        if  training_iter>0 and i==0:\n",
    "            lr = lr*0.5\n",
    "            optim = Adam(lr=lr, params=filter(lambda x: x.requires_grad, model.parameters()),weight_decay=5e-4)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        support_ways_text,support_ways_text_lens,query_ways_text,query_ways_text_lens = \\\n",
    "            generate_support_query(intent2text, intent_texts_len, sample_classes, way, shot, query)\n",
    "\n",
    "        loss = model(support_ways_text.t(),support_ways_text_lens,query_ways_text.t(),query_ways_text_lens,i)\n",
    "\n",
    "        if i%500==0:        \n",
    "            logging.debug('---------------')\n",
    "            logging.debug('iter %d loss %d'%(i,loss))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if i%500==0:\n",
    "            loss = 0\n",
    "            for j in range(len(test_sample_classes)):\n",
    "                support_ways_text,support_ways_text_lens,query_ways_text,query_ways_text_lens = \\\n",
    "                    generate_support_query(t_intent2text, t_intent_texts_len, test_sample_classes[j], way, shot, query)\n",
    "                loss += model(support_ways_text.t(),support_ways_text_lens,query_ways_text.t(),query_ways_text_lens,j)\n",
    "            loss/=len(test_sample_classes)\n",
    "            logging.debug('iter %d val_loss %d'%(i,loss))\n",
    "            torch.save(model.state_dict(), PATH + '/' + str(i) + '.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "\n",
    "model = Fewshot(num_words=len(word2id),embed_size=emb,hidden_size=hid,emb_weight =emb_weights,\n",
    "                   way=way,shot=shot,query=query,dynamic_iter=dynamic_iter,k=k,d_a=d_a)\n",
    "PATH='./model_new/10000.pt'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.cuda()\n",
    "\n",
    "n_Classes_vectors = []\n",
    "for i in range(5): #试情况而定，取均值\n",
    "    support_idx, query_idx = {},{}\n",
    "    support_ways_text,support_ways_text_lens = [],[]\n",
    "    query_ways_text,query_ways_text_lens = [],[]\n",
    "\n",
    "    support_idx = all_sq(intent2text,intent_texts_len,way,shot) \n",
    "\n",
    "    with open('id2intent','rb') as f:\n",
    "        id2intent = pickle.load(f)  #int2str\n",
    "    with open('id2word','rb') as f:\n",
    "        id2word = pickle.load(f)\n",
    "\n",
    "    support_ways_text,support_ways_text_lens = dict_2_2array(support_idx)\n",
    "    support_ways_text = support_ways_text.cuda()#[140*shot,text_length]\n",
    "    support_ways_text_lens = support_ways_text_lens#.cuda()\n",
    "    n_Classes_vectors.append(model.get_classes_vectors(support_ways_text.t(),support_ways_text_lens).squeeze())\n",
    "    \n",
    "    #[classes, 256]\n",
    "Classes_vector = n_Classes_vectors[0]+n_Classes_vectors[1]+n_Classes_vectors[2]+n_Classes_vectors[3]+n_Classes_vectors[4]\n",
    "print(Classes_vector.size())\n",
    "with open('Classes_vectors','wb') as f:\n",
    "    pickle.dump(Classes_vector,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "df = pd.read_excel('./gongan_test.xlsx')\n",
    "df.head()\n",
    "columns = df.columns\n",
    "test_text_list, test_label_list = [],[]\n",
    "values = df[columns[0]].values\n",
    "for idx, item in enumerate(values):\n",
    "    test_text_list.append(item)\n",
    "values = df[columns[1]].values\n",
    "for idx, item in enumerate(values):\n",
    "    test_label_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model\n",
    "from Untitled1 import Fewshot\n",
    "import jieba\n",
    "max_len = 20\n",
    "#query_text = '请问客服在吗,我找他？','好的'\n",
    "query_text = tuple(test_text_list)\n",
    "with open('word2id','rb') as f:\n",
    "    word2id = pickle.load(f)\n",
    "query = []\n",
    "query_lens = []\n",
    "\n",
    "for text_i in query_text:\n",
    "    text_i = jieba.cut(text_i,cut_all=False)\n",
    "    text_i = (list(text_i))\n",
    "    text_i = [word2id[w] if (w in word2id) else (1) for w in text_i]\n",
    "    l = len(text_i) \n",
    "    if l<20:\n",
    "        text_i.extend([0] * (max_len - l))\n",
    "    else: \n",
    "        text_i = text_i[:20]\n",
    "    query.append(text_i)#[q_nums,max_len]\n",
    "    if l<20: after_l = l\n",
    "    else: after_l=20\n",
    "    query_lens.append(after_l)\n",
    "#query构建完毕\n",
    "query = torch.tensor(query).cuda()\n",
    "query_lens = torch.tensor(query_lens)#.cuda()\n",
    "\n",
    "q_scores = model.get_classify_result(Classes_vector, query.t(),query_lens)\n",
    "print(q_scores.size())\n",
    "result_index = torch.max(q_scores, 0)[1]\n",
    "q_scores = q_scores.t()\n",
    "\n",
    "#对应classes：\n",
    "\n",
    "with open('id2intent','rb') as f:\n",
    "    id2intent = pickle.load(f)\n",
    "\n",
    "intent_class_vector = [id2intent[int(i)] for i in intent2text.keys()]\n",
    "\n",
    "   \n",
    "q_classes = [id2intent[i.item()] for i in result_index]\n",
    "\n",
    "num_r = 0\n",
    "for i in range(len(test_text_list)):  \n",
    "    if q_classes[i]!=test_label_list[i]:  \n",
    "        #print('---------------------------------')   \n",
    "        #print(query_text[i])\n",
    "        #print('预测', q_classes[i],'真值',test_label_list[i])\n",
    "        above095 = [j for j in range(32) if q_scores[i][j]>0.95]\n",
    "        #above095.sort(reverse=True)\n",
    "        #print('所有可能答案-----')\n",
    "        #for k in above095: print(intent_class_vector[k],q_scores[i][k])\n",
    "        \n",
    "        \n",
    "        num_r+=1\n",
    "print(num_r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
