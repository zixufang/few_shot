{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch            \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    #实现，每一个intent下面有多少个同义句\n",
    "    intentlist = []#所有的intent集合\n",
    "    for item in data:\n",
    "        if not item['intent'] in intentlist:\n",
    "            intentlist.append(item['intent'])\n",
    "            \n",
    "    textlist = [0 for i in range(len(intentlist))]\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['intent'] in intentlist:\n",
    "            textlist[intentlist.index(data[i]['intent'])] += 1 \n",
    "    \n",
    "    #把同义句低于5个的intent包含部分删除，intentlist包含的是所有的intent集合，textlist：intentlist对应项的同义句个数\n",
    "    del_data = []\n",
    "    for item in data:\n",
    "        if (textlist[intentlist.index(item['intent'])] >4):\n",
    "            del_data.append(item)\n",
    "        else:\n",
    "            textlist[intentlist.index(item['intent'])]  = 0\n",
    "    print(len(del_data))\n",
    "    return del_data, intentlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id(data):   \n",
    "    intent2id = {}\n",
    "    id2intent = {}\n",
    "    for item in data:\n",
    "        if not item['intent'] in intent2id:\n",
    "            len_intent2id = len(intent2id)\n",
    "            intent2id[item['intent']] = len_intent2id\n",
    "            id2intent[len_intent2id] = item['intent']\n",
    "    with open(\"intent2id\",'wb') as f:\n",
    "            pickle.dump(intent2id,f) \n",
    "    with open(\"id2intent\",'wb') as f:\n",
    "            pickle.dump(id2intent,f) \n",
    "\n",
    "    word2id={}\n",
    "    word2id['PAD'] = 0\n",
    "    word2id['UNK'] = 1\n",
    "    id2word={}\n",
    "    id2word[0] = 'PAD'\n",
    "    id2word[1] = 'UNK'\n",
    "    for item in data:\n",
    "        seg_text = jieba.cut(item['text'],cut_all=False)\n",
    "        seg_text = (list(seg_text))   \n",
    "        for w in seg_text:\n",
    "            if w not in word2id:\n",
    "                len_word2id = len(word2id) \n",
    "                word2id[w] = len_word2id\n",
    "                id2word[len_word2id] = w\n",
    "        item['text']=seg_text#使用结巴分词已经划分好\n",
    "    with open(\"word2id\",'wb') as f:\n",
    "            pickle.dump(word2id,f) \n",
    "    with open(\"id2word\",'wb') as f:\n",
    "            pickle.dump(id2word,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_word2vec(emb_size):\n",
    "    with open('word2id','rb') as f:\n",
    "        word2id = pickle.load(f)  #2889个\n",
    "    f.close()\n",
    "    weights = np.zeros([len(word2id),emb_size], dtype=np.float32)\n",
    "    \n",
    "    word2vec = {}\n",
    "    lines_num = 0\n",
    "    with open('financial_bigram-char',errors='ignore') as f:\n",
    "        first_line = True\n",
    "        for line in f:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                dim = int(line.rstrip().split()[1])\n",
    "                continue\n",
    "            lines_num += 1\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            word2vec[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "\n",
    "    for word_i in word2id:\n",
    "        if word_i in word2vec:\n",
    "            weights[word2id[word_i]] = word2vec[word_i]\n",
    "        else:\n",
    "            weights[word2id[word_i]] = np.random.uniform(-0.1,0.1)\n",
    "    weights = torch.from_numpy(weights)\n",
    "    print(weights.shape)\n",
    "    with open('renmin_emb_weights','wb') as g:\n",
    "        pickle.dump(weights,g) \n",
    "    return weights\n",
    "#load_pretrained_word2vec(300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def sample_sq(intent2text,intent_texts_len,sample_classes,way,shot,query):\n",
    "    \n",
    "    \n",
    "    support_idx, query_idx = {},{}\n",
    "\n",
    "    for class_i in sample_classes: \n",
    "        sample_set = []\n",
    "        sample_set = intent2text[str(class_i)]#二维数组.intent对应的所有text\n",
    "        text_content,text_lens ,sample_s_nums = [],[],[]\n",
    "        \n",
    "        #sample_s_nums找出intent2text某一个intent（sample_set）下面，数组选哪几个位置的text，位置集合\n",
    "        sample_s_nums = np.random.permutation(len(sample_set))[:shot] \n",
    "        text_content = [sample_set[item] for item in sample_s_nums]\n",
    "        text_lens = [intent_texts_len[str(class_i)][num] for num in sample_s_nums]\n",
    "        support_idx[str(class_i)] = {'text_content':text_content,'text_len':text_lens}\n",
    "       \n",
    "        #query_set构建\n",
    "        text_content,text_lens ,sample_query_nums,sample_q_nums = [],[],[],[]\n",
    "        sample_query_nums = [i for i in range(len(sample_set)) if i not in sample_s_nums]#集合减小\n",
    "        sample_q_nums = np.random.permutation(sample_query_nums)[:query]\n",
    "        text_content = [sample_set[item] for item in sample_q_nums]\n",
    "        text_lens = [intent_texts_len[str(class_i)][num] for num in sample_q_nums]\n",
    "        query_idx[str(class_i)] = {'text_content':text_content,'text_len':text_lens}\n",
    "    return support_idx, query_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_2_2array(x):\n",
    "    sq_idx = x\n",
    "    \n",
    "    sq_ways_text = None #把所有classes的text集合成一个二维数组\n",
    "    for i in sq_idx:\n",
    "        if sq_ways_text==None:\n",
    "            sq_ways_text = sq_idx[i]['text_content'] \n",
    "        else:\n",
    "            sq_ways_text.extend(sq_idx[i]['text_content'])\n",
    "      \n",
    "    \n",
    "    sq_ways_text_lens = None\n",
    "    for i in sq_idx:\n",
    "        if sq_ways_text_lens==None:\n",
    "            sq_ways_text_lens = sq_idx[i]['text_len'] \n",
    "        else:\n",
    "            sq_ways_text_lens.extend(sq_idx[i]['text_len'])\n",
    "    \n",
    "    sq_ways_text = torch.tensor(sq_ways_text)\n",
    "    sq_ways_text_lens = torch.tensor(sq_ways_text_lens)\n",
    "    #print(sq_ways_text.size())\n",
    "    return sq_ways_text,sq_ways_text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建最终数据,生成传入模型的intent2text，intent_texts_len\n",
    "def generate_intent2text_intent_texts_len(data):\n",
    "    intent2text = {}\n",
    "    with open('id2word','rb') as f:\n",
    "        id2word = pickle.load(f)\n",
    "    with open('id2intent','rb') as f:\n",
    "        id2intent = pickle.load(f)\n",
    "    for item in (data):\n",
    "        i = item['intent']\n",
    "        j = item['text']\n",
    "        if str(i) in intent2text:\n",
    "            intent2text[str(i)].append(j)\n",
    "        else:\n",
    "            intent2text[str(i)] = [j]    \n",
    "        \n",
    "   #重复某些句子以扩充到30个，多的删减到100个\n",
    "    for item in intent2text: \n",
    "        if len(intent2text[item])<30:#\n",
    "            l_past = len(intent2text[item])\n",
    "            a = intent2text[item]\n",
    "            extra = np.random.permutation(a)[:(30-l_past)].tolist()\n",
    "            intent2text[item].extend(extra)\n",
    "        elif len(intent2text[item])>100:#\n",
    "            l_past = len(intent2text[item])\n",
    "            a = intent2text[item]\n",
    "            intent2text[item] = np.random.permutation(a)[:100].tolist()   \n",
    "\n",
    "    intent_texts_len = {}#生成句子对应的长度\n",
    "    for item in intent2text:\n",
    "        texts = intent2text[item]\n",
    "        for text_i in texts:\n",
    "            if 0 in text_i: index_0 = text_i.index(0)\n",
    "            else: index_0 = 19#找到长度\n",
    "            if str(item) in intent_texts_len:\n",
    "                intent_texts_len[str(item)].append(index_0+1)\n",
    "            else:\n",
    "                intent_texts_len[str(item)] = [(index_0+1)]          \n",
    "    return intent2text, intent_texts_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_support_query(intent2text, intent_texts_len, sample_classes, way, shot, query):\n",
    "    support_idx, query_idx = {},{}\n",
    "    support_ways_text,support_ways_text_lens = [],[]\n",
    "    query_ways_text,query_ways_text_lens = [],[]\n",
    "    support_idx, query_idx = sample_sq(intent2text,intent_texts_len,sample_classes, way,shot,query)  \n",
    "    support_ways_text,support_ways_text_lens = dict_2_2array(support_idx)\n",
    "    query_ways_text,query_ways_text_lens = dict_2_2array(query_idx)\n",
    "    support_ways_text = support_ways_text.cuda()\n",
    "    support_ways_text_lens = support_ways_text_lens#.cuda()\n",
    "    query_ways_text = query_ways_text.cuda()\n",
    "    query_ways_text_lens = query_ways_text_lens#.cuda()\n",
    "    return support_ways_text,support_ways_text_lens,query_ways_text,query_ways_text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_gru(gru):\n",
    "    gru.reset_parameters()\n",
    "    for _, hh, _, _ in gru.all_weights:\n",
    "        for i in range(0, hh.size(0), gru.hidden_size):\n",
    "            nn.init.orthogonal_(hh[i:i + gru.hidden_size], gain=1)\n",
    "\n",
    "\n",
    "\n",
    "#实现model的三个环节\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, n_layers, weight):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_layers = n_layers\n",
    "        #self.dropout = dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, bidirectional=True)\n",
    "        init_gru(self.gru)\n",
    "\n",
    "    def forward(self, input_seqs, input_lens, hidden=None):\n",
    "        \"\"\"\n",
    "        forward procedure. No need for inputs to be sorted\n",
    "        :param input_seqs: Variable of [T,B]\n",
    "        :param hidden:\n",
    "        :param input_lens: *numpy array* of len for each input sequence\n",
    "        :return:\n",
    "        \"\"\"      \n",
    "        batch_size = input_seqs.size(1)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        embedded = embedded.transpose(0, 1)  # [B,T,E]\n",
    "      \n",
    "        sort_idx = np.argsort(-input_lens)\n",
    "        #DELETE = cuda_(torch\n",
    "        unsort_idx = (torch.LongTensor(np.argsort(sort_idx))).cuda()\n",
    "        input_lens = input_lens[sort_idx]\n",
    "        #DELETE = cuda_(torch\n",
    "        sort_idx = (torch.LongTensor(sort_idx)).cuda()\n",
    "        embedded = embedded[sort_idx].transpose(0, 1)  # [T,B,E]\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lens)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        #print(outputs.size(),'encoder')\n",
    "        outputs = outputs.transpose(0, 1)[unsort_idx].transpose(0, 1).contiguous()\n",
    "        hidden = hidden.transpose(0, 1)[unsort_idx].transpose(0, 1).contiguous()\n",
    "        return outputs, hidden, embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size,d_a):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_a = d_a\n",
    "        self.W2_weights = nn.Parameter(torch.Tensor(1, self.d_a))\n",
    "        torch.nn.init.xavier_uniform(self.W2_weights.data)\n",
    "\n",
    "        self.W1 = (nn.Linear(self.hidden_size*2,self.d_a))\n",
    "        \n",
    "    def forward(self,encoder_outputs,output_batch):\n",
    "        output = encoder_outputs\n",
    "        #encoder_outputs--[length,batch,hidden*2]      \n",
    "        W1 = torch.tanh(self.W1(output)) #[length,batch,da]\n",
    "        W1 = W1.permute(1,0,2)#[batch,length,da]\n",
    "        score = torch.bmm(W1,\n",
    "                            self.W2_weights  # (1, da)\n",
    "                            .permute(1, 0)  # (da, 1)\n",
    "                            .unsqueeze(0)  # (1, da, 1)\n",
    "                            .repeat(output_batch, 1, 1)\n",
    "                            # (batch_size, da, 1)\n",
    "                            )\n",
    "        score = F.softmax(F.relu(score.squeeze()))\n",
    "        weighted = torch.mul(output.permute(1,0,2), score.unsqueeze(-1).expand_as(output.permute(1,0,2)))\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "        return representations\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Dynamic_route(nn.Module):\n",
    "    def __init__(self,n_iter,hidden_size):\n",
    "        super(Dynamic_route,self).__init__()\n",
    "        self.n_iter = n_iter\n",
    "        self.embedding = 2*hidden_size\n",
    "        self.trans_W = nn.Linear(self.embedding,self.embedding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.trans_W(x)#[way,shot,vector_embedding]\n",
    "        logits = torch.zeros(x.size(0),x.size(1),1).cuda()                                                \n",
    "        for i in range(self.n_iter):\n",
    "            probs = F.softmax(logits,dim=1)\n",
    "            #print(probs.size())\n",
    "            probs_mul_x = torch.mul(x, probs.expand_as(x))\n",
    "            y = self.squash(probs_mul_x.sum(dim=1,keepdim=True))##[way,1,vector_embedding]\n",
    "            if i!= self.n_iter-1:\n",
    "                delta_logits = (y * x).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        return y  \n",
    "        \n",
    "    def squash(self, tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Relation_score(nn.Module):\n",
    "    def __init__(self,k, batch, in_channel, out_channel):\n",
    "        super(Relation_score,self).__init__()\n",
    "        self.neural_tensor_net = nn.Bilinear(in_channel,out_channel,k)\n",
    "        self.mlp = (nn.Linear(k,1))\n",
    "    def forward(self,query,classes):\n",
    "        \n",
    "        relation_vector = self.neural_tensor_net(query,classes)\n",
    "        final_score = F.sigmoid(self.mlp(relation_vector))\n",
    "        #print(relation_vector.size())       \n",
    "        return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fewshot(nn.Module):\n",
    "    def __init__(self, num_words, embed_size, hidden_size, emb_weight, way, shot, query, \n",
    "                 dynamic_iter,k, d_a, **kwargs):\n",
    "        super(Fewshot, self).__init__()\n",
    "        self.input_size = num_words\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_weights = emb_weight\n",
    "        self.way = way\n",
    "        self.shot = shot\n",
    "        self.query = query\n",
    "        self.k = k\n",
    "        self.encoder = Encoder(self.input_size,self.embed_size,self.hidden_size,weight=self.emb_weights,n_layers=1)\n",
    "        self.att = Attn(self.hidden_size, d_a)\n",
    "        self.dynamic_route = Dynamic_route(dynamic_iter,self.hidden_size)\n",
    "        self.relation_score = Relation_score(k, way*query, 2*self.hidden_size, 2*self.hidden_size)\n",
    "        \n",
    "    def forward(self, support_text,support_text_len,query_text,query_text_len,train_i):\n",
    "        output,hidden,_ = self.encoder(support_text,support_text_len)\n",
    "        weighted_rep = self.att(output,output.size(1))\n",
    "        C_K = weighted_rep.view(self.way, self.shot, self.hidden_size * 2)\n",
    "        Classes_vectors = self.dynamic_route(C_K)\n",
    "        \n",
    "        output,hidden,_ = self.encoder(query_text,query_text_len)\n",
    "        query_vectors = self.att(output,output.size(1))\n",
    "        ex = Classes_vectors\n",
    "        #print(Classes_vectors.size(),'Classes_vectors')#[5, 1, 256]\n",
    "        \n",
    "        loss = torch.tensor(0.0).cuda()\n",
    "        for i in range(self.way):\n",
    "            groud_truth = [0.0 for i in range(self.way * self.query)]\n",
    "            groud_truth[self.query * i : self.query * (i+1)] = [1.0 for i in range(self.query)]\n",
    "            groud_truth = torch.tensor(groud_truth).cuda()\n",
    "            #print(query_vectors.size())#([50, 256])\n",
    "            \n",
    "            score = self.relation_score(query_vectors,ex[i].repeat(self.way*self.query,1)).squeeze()#type,tensor\n",
    "            #if train_i%10000==0:\n",
    "                #print(score[0:20],'-----------',score[20:40],'----------',score[40:60],'----------',\n",
    "                     #score[60:80],'-----------',score[80:100])\n",
    "                #print('#############')\n",
    "            #print(score.size())[50]\n",
    "            loss += ((score - groud_truth)**2).sum()\n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    def get_classes_vectors(self, support_text,support_text_len):#注意输入的时候 t.()\n",
    "        output,hidden,_ = self.encoder(support_text,support_text_len)\n",
    "        weighted_rep = self.att(output,output.size(1))\n",
    "        C_K = weighted_rep.view(int(float(support_text_len.size(0))/self.shot), self.shot, self.hidden_size * 2)\n",
    "        Classes_vectors = self.dynamic_route(C_K)\n",
    "        return Classes_vectors\n",
    "    \n",
    "    def get_classify_result(self,Classes_vectors,query_text,query_text_len):\n",
    "        with open('id2intent','rb') as f:\n",
    "            id2intent = pickle.load(f)\n",
    "\n",
    "        output,hidden,_ = self.encoder(query_text,query_text_len)\n",
    "        query_vectors = self.att(output,output.size(1))\n",
    "        res_socres = []\n",
    "        for i in range(Classes_vectors.size(0)):  \n",
    "            score_i = self.relation_score(query_vectors,Classes_vectors[i].repeat(query_vectors.size(0),1)).squeeze()#type,tensor\n",
    "            res_socres.append(score_i)\n",
    "        \n",
    "        #print(type(res_socres))\n",
    "        res_socres = torch.stack(res_socres,dim=0)\n",
    "        #print(type(res_socres))\n",
    "        return res_socres\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def all_sq(intent2text,intent_texts_len,way,shot):\n",
    "      \n",
    "    train_classes = list(intent2text.keys())\n",
    "    \n",
    "    support_idx, query_idx = {},{}\n",
    "    #本来是输出5个class的信息，\n",
    "\n",
    "    for class_i in train_classes: \n",
    "        sample_set = []\n",
    "        sample_set = intent2text[str(class_i)]#二维数组.intent对应的所有text\n",
    "        text_content,text_lens ,sample_s_nums = [],[],[]\n",
    "        \n",
    "        #sample_s_nums找出intent2text某一个intent（sample_set）下面，数组选哪几个位置的text，位置集合\n",
    "        sample_s_nums = np.random.permutation(len(sample_set))[:shot] \n",
    "        text_content = [sample_set[item] for item in sample_s_nums]\n",
    "        text_lens = [intent_texts_len[str(class_i)][num] for num in sample_s_nums]\n",
    "        support_idx[str(class_i)] = {'text_content':text_content,'text_len':text_lens}\n",
    "\n",
    "    return support_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
