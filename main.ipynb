{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Untitled1.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pickle\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import json\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import statsmodels.api as sm \n",
    "import random \n",
    "import jieba\n",
    "import Ipynb_importer\n",
    "import Untitled1\n",
    "from Untitled1 import load_pretrained_word2vec\n",
    "#Encoder, Attn, Dynamic_route, Relation_score\n",
    "\n",
    "from Untitled1 import dict_2_2array, sample_sq, Fewshot\n",
    "\n",
    "\n",
    "with open('xinwang_train.json') as f:\n",
    "    load_json = json.load(f)\n",
    "    data = load_json['rasa_nlu_data']['common_examples']\n",
    "for i in range(len(data)):\n",
    "    tep = data[i]['intent']\n",
    "    data[i]['intent'] = data[i]['text']\n",
    "    data[i]['text'] = tep\n",
    "#删除重复项\n",
    "res_data = []\n",
    "for item in data:\n",
    "    if not item in res_data:\n",
    "        res_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "intentlist = []\n",
    "for item in res_data:\n",
    "    if not item['intent'] in intentlist:\n",
    "        intentlist.append(item['intent'])\n",
    "textlist = [0 for i in range(len(intentlist))]\n",
    "for i in range(len(res_data)):\n",
    "    if res_data[i]['intent'] in intentlist:\n",
    "        textlist[intentlist.index(res_data[i]['intent'])] += 1 \n",
    "        #每一个intent下面有多少个同义句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8176\n"
     ]
    }
   ],
   "source": [
    "del_data = []\n",
    "for item in res_data:\n",
    "    if (textlist[intentlist.index(item['intent'])] >4):\n",
    "        del_data.append(item)\n",
    "    else:\n",
    "        textlist[intentlist.index(item['intent'])]  = 0\n",
    "print(len(del_data))\n",
    "data = del_data\n",
    "#把同义句低于5个的intent包含部分删除，intent包含的是所有的intent集合，text包含的是intent集合对应项的同义句的个数，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/fangting/anaconda3/envs/ft/lib/python3.6/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpeyr2txox' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.807 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "intent2id = {}\n",
    "for item in data:\n",
    "    if not item['intent'] in intent2id:\n",
    "        intent2id[item['intent']] = len(intent2id)\n",
    "print(len(intent2id))\n",
    "with open(\"intent2id\",'wb') as f:\n",
    "        pickle.dump(intent2id,f) \n",
    "        \n",
    "word2id={}\n",
    "word2id['PAD'] = 0\n",
    "word2id['UNK'] = 1\n",
    "for item in data:\n",
    "    seg_text = jieba.cut(item['text'],cut_all=False)\n",
    "    seg_text = (list(seg_text))   \n",
    "    for w in seg_text:\n",
    "        if w not in word2id:\n",
    "            word2id[w] = len(word2id)        \n",
    "    item['text']=seg_text#使用结巴分词已经划分好\n",
    "with open(\"word2id\",'wb') as f:\n",
    "        pickle.dump(word2id,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': '可以咨询吗？', 'text': ['人工', '服务']}\n",
      "{'intent': '可以咨询吗？', 'text': ['人工', '客服']}\n"
     ]
    }
   ],
   "source": [
    "data1 = data\n",
    "for item in data1:\n",
    "    for w in item['text']:\n",
    "        if w not in word2id:\n",
    "            w = 'UNK'\n",
    "for i in range(2):\n",
    "    print(data1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转化成数字label\n",
    "with open('intent2id','rb') as f:\n",
    "    intent2id = pickle.load(f)\n",
    "with open('word2id','rb') as f:\n",
    "    word2id = pickle.load(f)   \n",
    "for item in data1:\n",
    "    #print(intent2id[item['intent']])\n",
    "    item['intent'] = intent2id[item['intent']]\n",
    "    item['text'] = [word2id[w] for w in item['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查text的长度分布,截断长度\n",
    "text_len = [0 for i in range(100)]\n",
    "for item in data1:\n",
    "    if(len(item['text']))<100:\n",
    "        text_len[len(item['text'])]+=1\n",
    "#print(text_len)\n",
    "max_len = 20\n",
    "for item in data1:\n",
    "    length = len(item['text'])\n",
    "    if length<20:\n",
    "        item['text'].extend([0] * (max_len - length))\n",
    "    else:\n",
    "        item['text'] = item['text'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data1)\n",
    "train_data = data1[:7300]\n",
    "test_data = data1[7300:]\n",
    "#可用的有len_data，便于lstm进行切分。data1。word2id()，intent2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#做一个intent的同义句集合(intent_same记录同义句数量)，集合i位置代表intent_i的同义句有多少个，。筛选intent同义句大于15个的data条数，\n",
    "intent_same = [0 for i in range(355)]\n",
    "for item in train_data:\n",
    "    intent_same[item['intent']] += 1\n",
    "#print(intent_same)\n",
    "#train_data1筛选出intent同义句>15\n",
    "train_data1 = []\n",
    "for item in train_data:\n",
    "    if intent_same[item['intent']]>15:\n",
    "        train_data1.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建最终数据\n",
    "intent2text = {}\n",
    "intent_texts_len = {}\n",
    "for item in (train_data1):\n",
    "    i = item['intent']\n",
    "    j = item['text']\n",
    "    if str(i) in intent2text:\n",
    "        intent2text[str(i)].append(j)\n",
    "        intent_texts_len[str(i)].append(len(j))\n",
    "    else:\n",
    "        intent2text[str(i)] = [j]\n",
    "        intent_texts_len[str(i)] = [len(j)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在每个episode，挑选5个class，每个class中选5个作为example，10个作为query，\n",
    "#可用的有train_data1,intent2text(dict，intent作为key（char），text是一个二维数组),intent_texts_len\n",
    "#print(intent_texts_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eposide_iter = 50000\n",
    "way = 5\n",
    "shot = 5\n",
    "query = 10\n",
    "dynamic_iter = 3\n",
    "k = 100\n",
    "d_a = 64\n",
    "emb = 300\n",
    "hid=128\n",
    "lr =  0.001\n",
    "PATH = './model/'\n",
    "#print(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb_weights','rb') as f:\n",
    "    emb_weights = pickle.load(f)\n",
    "with open('word2id','rb') as f:\n",
    "    word2id = pickle.load(f)  #2889个词\n",
    "#for i in range(eposide_iter):\n",
    "from Untitled1 import sample_sq, dict_2_2array\n",
    "support_idx, query_idx = {},{}\n",
    "#support_idx, query_idx = sample_sq(intent2text,intent_texts_len,way,shot,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from Untitled1 import dict_2_2array, sample_sq, Fewshot\n",
    "from torch.optim import Adam\n",
    "model = Fewshot(num_words=len(word2id),embed_size=emb,hidden_size=hid,emb_weight =emb_weights,\n",
    "                   way=way,shot=shot,query=query,dynamic_iter=dynamic_iter,k=k,d_a=d_a)\n",
    "model.cuda()\n",
    "optim = Adam(lr=lr, params=filter(lambda x: x.requires_grad, model.parameters()),weight_decay=5e-4)\n",
    "\n",
    "for i in range(eposide_iter): \n",
    "    if (i+1)%300==0 and i<=2000:\n",
    "        lr = lr*0.7\n",
    "        optim = Adam(lr=lr, params=filter(lambda x: x.requires_grad, model.parameters()),weight_decay=5e-4)\n",
    "    optim.zero_grad()\n",
    "    support_idx, query_idx = {},{}\n",
    "    support_ways_text,support_ways_text_lens = [],[]\n",
    "    query_ways_text,query_ways_text_lens = [],[]\n",
    "        \n",
    "    support_idx, query_idx = sample_sq(intent2text,intent_texts_len,way,shot,query)  \n",
    "    support_ways_text,support_ways_text_lens = dict_2_2array(support_idx)\n",
    "    query_ways_text,query_ways_text_lens = dict_2_2array(query_idx)\n",
    "    support_ways_text = support_ways_text.cuda()\n",
    "    support_ways_text_lens = support_ways_text_lens#.cuda()\n",
    "    query_ways_text = query_ways_text.cuda()\n",
    "    query_ways_text_lens = query_ways_text_lens#.cuda()\n",
    "    \n",
    "    loss = model(support_ways_text.t(),support_ways_text_lens,query_ways_text.t(),query_ways_text_lens)\n",
    "    \n",
    "    if i%500==0:\n",
    "        print(loss)\n",
    "    if (i%100==0):\n",
    "        print(i,'------------------------')\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i%50==0:\n",
    "        torch.save(model.state_dict(), PATH+str(i)+'.pt')\n",
    "    "
   ]
  },
  
