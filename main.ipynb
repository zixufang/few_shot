{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Untitled1.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pickle\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import json\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import statsmodels.api as sm \n",
    "import random \n",
    "import jieba\n",
    "import Ipynb_importer\n",
    "import Untitled1\n",
    "from Untitled1 import load_pretrained_word2vec\n",
    "#Encoder, Attn, Dynamic_route, Relation_score\n",
    "\n",
    "from Untitled1 import dict_2_2array, sample_sq, Fewshot\n",
    "\n",
    "\n",
    "with open('xinwang_train.json') as f:\n",
    "    load_json = json.load(f)\n",
    "    data = load_json['rasa_nlu_data']['common_examples']\n",
    "for i in range(len(data)):\n",
    "    tep = data[i]['intent']\n",
    "    data[i]['intent'] = data[i]['text']\n",
    "    data[i]['text'] = tep\n",
    "#删除重复项\n",
    "res_data = []\n",
    "for item in data:\n",
    "    if not item in res_data:\n",
    "        res_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "intentlist = []\n",
    "for item in res_data:\n",
    "    if not item['intent'] in intentlist:\n",
    "        intentlist.append(item['intent'])\n",
    "textlist = [0 for i in range(len(intentlist))]\n",
    "for i in range(len(res_data)):\n",
    "    if res_data[i]['intent'] in intentlist:\n",
    "        textlist[intentlist.index(res_data[i]['intent'])] += 1 \n",
    "        #每一个intent下面有多少个同义句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8176\n"
     ]
    }
   ],
   "source": [
    "del_data = []\n",
    "for item in res_data:\n",
    "    if (textlist[intentlist.index(item['intent'])] >4):\n",
    "        del_data.append(item)\n",
    "    else:\n",
    "        textlist[intentlist.index(item['intent'])]  = 0\n",
    "print(len(del_data))\n",
    "data = del_data\n",
    "#把同义句低于5个的intent包含部分删除，intent包含的是所有的intent集合，text包含的是intent集合对应项的同义句的个数，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/fangting/anaconda3/envs/ft/lib/python3.6/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpeyr2txox' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.807 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "intent2id = {}\n",
    "for item in data:\n",
    "    if not item['intent'] in intent2id:\n",
    "        intent2id[item['intent']] = len(intent2id)\n",
    "print(len(intent2id))\n",
    "with open(\"intent2id\",'wb') as f:\n",
    "        pickle.dump(intent2id,f) \n",
    "        \n",
    "word2id={}\n",
    "word2id['PAD'] = 0\n",
    "word2id['UNK'] = 1\n",
    "for item in data:\n",
    "    seg_text = jieba.cut(item['text'],cut_all=False)\n",
    "    seg_text = (list(seg_text))   \n",
    "    for w in seg_text:\n",
    "        if w not in word2id:\n",
    "            word2id[w] = len(word2id)        \n",
    "    item['text']=seg_text#使用结巴分词已经划分好\n",
    "with open(\"word2id\",'wb') as f:\n",
    "        pickle.dump(word2id,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': '可以咨询吗？', 'text': ['人工', '服务']}\n",
      "{'intent': '可以咨询吗？', 'text': ['人工', '客服']}\n"
     ]
    }
   ],
   "source": [
    "data1 = data\n",
    "for item in data1:\n",
    "    for w in item['text']:\n",
    "        if w not in word2id:\n",
    "            w = 'UNK'\n",
    "for i in range(2):\n",
    "    print(data1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转化成数字label\n",
    "with open('intent2id','rb') as f:\n",
    "    intent2id = pickle.load(f)\n",
    "with open('word2id','rb') as f:\n",
    "    word2id = pickle.load(f)   \n",
    "for item in data1:\n",
    "    #print(intent2id[item['intent']])\n",
    "    item['intent'] = intent2id[item['intent']]\n",
    "    item['text'] = [word2id[w] for w in item['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查text的长度分布,截断长度\n",
    "text_len = [0 for i in range(100)]\n",
    "for item in data1:\n",
    "    if(len(item['text']))<100:\n",
    "        text_len[len(item['text'])]+=1\n",
    "#print(text_len)\n",
    "max_len = 20\n",
    "for item in data1:\n",
    "    length = len(item['text'])\n",
    "    if length<20:\n",
    "        item['text'].extend([0] * (max_len - length))\n",
    "    else:\n",
    "        item['text'] = item['text'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data1)\n",
    "train_data = data1[:7300]\n",
    "test_data = data1[7300:]\n",
    "#可用的有len_data，便于lstm进行切分。data1。word2id()，intent2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#做一个intent的同义句集合(intent_same记录同义句数量)，集合i位置代表intent_i的同义句有多少个，。筛选intent同义句大于15个的data条数，\n",
    "intent_same = [0 for i in range(355)]\n",
    "for item in train_data:\n",
    "    intent_same[item['intent']] += 1\n",
    "#print(intent_same)\n",
    "#train_data1筛选出intent同义句>15\n",
    "train_data1 = []\n",
    "for item in train_data:\n",
    "    if intent_same[item['intent']]>15:\n",
    "        train_data1.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建最终数据\n",
    "intent2text = {}\n",
    "intent_texts_len = {}\n",
    "for item in (train_data1):\n",
    "    i = item['intent']\n",
    "    j = item['text']\n",
    "    if str(i) in intent2text:\n",
    "        intent2text[str(i)].append(j)\n",
    "        intent_texts_len[str(i)].append(len(j))\n",
    "    else:\n",
    "        intent2text[str(i)] = [j]\n",
    "        intent_texts_len[str(i)] = [len(j)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在每个episode，挑选5个class，每个class中选5个作为example，10个作为query，\n",
    "#可用的有train_data1,intent2text(dict，intent作为key（char），text是一个二维数组),intent_texts_len\n",
    "#print(intent_texts_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eposide_iter = 50000\n",
    "way = 5\n",
    "shot = 5\n",
    "query = 10\n",
    "dynamic_iter = 3\n",
    "k = 100\n",
    "d_a = 64\n",
    "emb = 300\n",
    "hid=128\n",
    "lr =  0.001\n",
    "PATH = './model/'\n",
    "#print(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb_weights','rb') as f:\n",
    "    emb_weights = pickle.load(f)\n",
    "with open('word2id','rb') as f:\n",
    "    word2id = pickle.load(f)  #2889个词\n",
    "#for i in range(eposide_iter):\n",
    "from Untitled1 import sample_sq, dict_2_2array\n",
    "support_idx, query_idx = {},{}\n",
    "#support_idx, query_idx = sample_sq(intent2text,intent_texts_len,way,shot,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(62.8413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0 ------------------------\n",
      "100 ------------------------\n",
      "200 ------------------------\n",
      "300 ------------------------\n",
      "400 ------------------------\n",
      "tensor(49.9999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "500 ------------------------\n",
      "600 ------------------------\n",
      "700 ------------------------\n",
      "800 ------------------------\n",
      "900 ------------------------\n",
      "tensor(39.4358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "1000 ------------------------\n",
      "1100 ------------------------\n",
      "1200 ------------------------\n",
      "1300 ------------------------\n",
      "1400 ------------------------\n",
      "tensor(37.5445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "1500 ------------------------\n",
      "1600 ------------------------\n",
      "1700 ------------------------\n",
      "1800 ------------------------\n",
      "1900 ------------------------\n",
      "tensor(26.7017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "2000 ------------------------\n",
      "2100 ------------------------\n",
      "2200 ------------------------\n",
      "2300 ------------------------\n",
      "2400 ------------------------\n",
      "tensor(26.9143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "2500 ------------------------\n",
      "2600 ------------------------\n",
      "2700 ------------------------\n",
      "2800 ------------------------\n",
      "2900 ------------------------\n",
      "tensor(26.8130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "3000 ------------------------\n",
      "3100 ------------------------\n",
      "3200 ------------------------\n",
      "3300 ------------------------\n",
      "3400 ------------------------\n",
      "tensor(24.5873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "3500 ------------------------\n",
      "3600 ------------------------\n",
      "3700 ------------------------\n",
      "3800 ------------------------\n",
      "3900 ------------------------\n",
      "tensor(17.3742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "4000 ------------------------\n",
      "4100 ------------------------\n",
      "4200 ------------------------\n",
      "4300 ------------------------\n",
      "4400 ------------------------\n",
      "tensor(17.5612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "4500 ------------------------\n",
      "4600 ------------------------\n",
      "4700 ------------------------\n",
      "4800 ------------------------\n",
      "4900 ------------------------\n",
      "tensor(16.2647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5000 ------------------------\n",
      "5100 ------------------------\n",
      "5200 ------------------------\n",
      "5300 ------------------------\n",
      "5400 ------------------------\n",
      "tensor(16.2387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5500 ------------------------\n",
      "5600 ------------------------\n",
      "5700 ------------------------\n",
      "5800 ------------------------\n",
      "5900 ------------------------\n",
      "tensor(8.0569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "6000 ------------------------\n",
      "6100 ------------------------\n",
      "6200 ------------------------\n",
      "6300 ------------------------\n",
      "6400 ------------------------\n",
      "tensor(16.1444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "6500 ------------------------\n",
      "6600 ------------------------\n",
      "6700 ------------------------\n",
      "6800 ------------------------\n",
      "6900 ------------------------\n",
      "tensor(8.5756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "7000 ------------------------\n",
      "7100 ------------------------\n",
      "7200 ------------------------\n",
      "7300 ------------------------\n",
      "7400 ------------------------\n",
      "tensor(22.2398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "7500 ------------------------\n",
      "7600 ------------------------\n",
      "7700 ------------------------\n",
      "7800 ------------------------\n",
      "7900 ------------------------\n",
      "tensor(12.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "8000 ------------------------\n",
      "8100 ------------------------\n",
      "8200 ------------------------\n",
      "8300 ------------------------\n"
     ]
    }
   ],
   "source": [
    "from Untitled1 import dict_2_2array, sample_sq, Fewshot\n",
    "from torch.optim import Adam\n",
    "model = Fewshot(num_words=len(word2id),embed_size=emb,hidden_size=hid,emb_weight =emb_weights,\n",
    "                   way=way,shot=shot,query=query,dynamic_iter=dynamic_iter,k=k,d_a=d_a)\n",
    "model.cuda()\n",
    "optim = Adam(lr=lr, params=filter(lambda x: x.requires_grad, model.parameters()),weight_decay=5e-4)\n",
    "\n",
    "for i in range(eposide_iter): \n",
    "    if (i+1)%300==0 and i<=2000:\n",
    "        lr = lr*0.7\n",
    "        optim = Adam(lr=lr, params=filter(lambda x: x.requires_grad, model.parameters()),weight_decay=5e-4)\n",
    "    optim.zero_grad()\n",
    "    support_idx, query_idx = {},{}\n",
    "    support_ways_text,support_ways_text_lens = [],[]\n",
    "    query_ways_text,query_ways_text_lens = [],[]\n",
    "        \n",
    "    support_idx, query_idx = sample_sq(intent2text,intent_texts_len,way,shot,query)  \n",
    "    support_ways_text,support_ways_text_lens = dict_2_2array(support_idx)\n",
    "    query_ways_text,query_ways_text_lens = dict_2_2array(query_idx)\n",
    "    support_ways_text = support_ways_text.cuda()\n",
    "    support_ways_text_lens = support_ways_text_lens#.cuda()\n",
    "    query_ways_text = query_ways_text.cuda()\n",
    "    query_ways_text_lens = query_ways_text_lens#.cuda()\n",
    "    \n",
    "    loss = model(support_ways_text.t(),support_ways_text_lens,query_ways_text.t(),query_ways_text_lens)\n",
    "    \n",
    "    if i%500==0:\n",
    "        print(loss)\n",
    "    if (i%100==0):\n",
    "        print(i,'------------------------')\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i%50==0:\n",
    "        torch.save(model.state_dict(), PATH+str(i)+'.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-15-5938b30bc76d>, line 28)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-5938b30bc76d>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    print(loss)\u001b[0m\n\u001b[0m               \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    '''\n",
    "    output,hidden,_ = encoder(support_ways_text.t(),support_ways_text_lens)\n",
    "    #output是(max_length, batch_size, hidden_size)（先前向后反向）\n",
    "    #hidden是(n_layers x num_directions, batch_size, hidden_size)\n",
    "    \n",
    "    att = Attn(hidden_size, d_a)\n",
    "    weighted_rep = att(output,output_batch)\n",
    "    C_K = weighted_rep.view(way,shot,hidden_size*2)\n",
    "    Classes_vectors = dynamic_route(C_K)\n",
    "    \n",
    "    output,hidden,_ = encoder(query_ways_text.t(),query_ways_text_lens)\n",
    "\n",
    "    query_vectors = att(output,output.size(1))\n",
    "    ex = Classes_vectors\n",
    "    ex1 = (ex.repeat(1,50,1))\n",
    "    k = 100\n",
    "    print(query_vectors.size(),ex1.size())\n",
    "\n",
    "    #query_batch = query_vectors.size(0)\n",
    "    #relation_score = Relation_score(100,query_batch,query_vectors.size(1),query_vectors.size(1))\n",
    "    loss = torch.tensor(0.0)\n",
    "    for i in range(way):\n",
    "        groud_truth = [0.0 for i in range(way*query)]\n",
    "        groud_truth[query*i:query*(i+1)] = [1.0 for i in range(query)]\n",
    "        groud_truth = torch.tensor(groud_truth)\n",
    "        score_i = relation_score(query_vectors,ex1[i]).squeeze()#type,tensor\n",
    "        loss += ((score_i - groud_truth)**2).sum()\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ft] *",
   "language": "python",
   "name": "conda-env-ft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
