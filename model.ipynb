{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch            \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_word2vec(emb_size):\n",
    "    with open('word2id','rb') as f:\n",
    "        word2id = pickle.load(f)  #2889个\n",
    "    f.close()\n",
    "    weights = np.zeros([len(word2id),emb_size], dtype=np.float32)\n",
    "    \n",
    "    word2vec = {}\n",
    "    lines_num = 0\n",
    "    with open('financial_bigram-char',errors='ignore') as f:\n",
    "        first_line = True\n",
    "        for line in f:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                dim = int(line.rstrip().split()[1])\n",
    "                continue\n",
    "            lines_num += 1\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            word2vec[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "\n",
    "    for word_i in word2id:\n",
    "        if word_i in word2vec:\n",
    "            weights[word2id[word_i]] = word2vec[word_i]\n",
    "        else:\n",
    "            weights[word2id[word_i]] = np.random.uniform(-0.1,0.1)\n",
    "    weights = torch.from_numpy(weights)\n",
    "    return weights\n",
    "#load_pretrained_word2vec(300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_gru(gru):\n",
    "    gru.reset_parameters()\n",
    "    for _, hh, _, _ in gru.all_weights:\n",
    "        for i in range(0, hh.size(0), gru.hidden_size):\n",
    "            nn.init.orthogonal_(hh[i:i + gru.hidden_size], gain=1)\n",
    "\n",
    "\n",
    "\n",
    "#实现model的三个环节\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, n_layers, weight):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_layers = n_layers\n",
    "        #self.dropout = dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, bidirectional=True)\n",
    "        init_gru(self.gru)\n",
    "\n",
    "    def forward(self, input_seqs, input_lens, hidden=None):\n",
    "        \"\"\"\n",
    "        forward procedure. No need for inputs to be sorted\n",
    "        :param input_seqs: Variable of [T,B]\n",
    "        :param hidden:\n",
    "        :param input_lens: *numpy array* of len for each input sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = input_seqs.size(1)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        embedded = embedded.transpose(0, 1)  # [B,T,E]\n",
    "        \n",
    "        sort_idx = np.argsort(-input_lens)\n",
    "        #DELETE = cuda_(torch\n",
    "        unsort_idx = (torch.LongTensor(np.argsort(sort_idx))).cuda()\n",
    "        input_lens = input_lens[sort_idx]\n",
    "        #DELETE = cuda_(torch\n",
    "        sort_idx = (torch.LongTensor(sort_idx)).cuda()\n",
    "        embedded = embedded[sort_idx].transpose(0, 1)  # [T,B,E]\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lens)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        #print(outputs.size())\n",
    "        outputs = outputs.transpose(0, 1)[unsort_idx].transpose(0, 1).contiguous()\n",
    "        hidden = hidden.transpose(0, 1)[unsort_idx].transpose(0, 1).contiguous()\n",
    "        return outputs, hidden, embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size,d_a):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_a = d_a\n",
    "        self.W2_weights = nn.Parameter(torch.Tensor(1, self.d_a))\n",
    "        torch.nn.init.xavier_uniform(self.W2_weights.data)\n",
    "\n",
    "        self.W1 = (nn.Linear(self.hidden_size*2,self.d_a))\n",
    "        \n",
    "    def forward(self,encoder_outputs,output_batch):\n",
    "        output = encoder_outputs\n",
    "        #encoder_outputs--[length,batch,hidden*2]      \n",
    "        W1 = torch.tanh(self.W1(output)) #[length,batch,da]\n",
    "        W1 = W1.permute(1,0,2)#[batch,length,da]\n",
    "        score = torch.bmm(W1,\n",
    "                            self.W2_weights  # (1, da)\n",
    "                            .permute(1, 0)  # (da, 1)\n",
    "                            .unsqueeze(0)  # (1, da, 1)\n",
    "                            .repeat(output_batch, 1, 1)\n",
    "                            # (batch_size, da, 1)\n",
    "                            )\n",
    "        score = F.softmax(F.relu(score.squeeze()))\n",
    "        weighted = torch.mul(output.permute(1,0,2), score.unsqueeze(-1).expand_as(output.permute(1,0,2)))\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "        return representations\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Dynamic_route(nn.Module):\n",
    "    def __init__(self,n_iter,hidden_size):\n",
    "        super(Dynamic_route,self).__init__()\n",
    "        self.n_iter = n_iter\n",
    "        self.embedding = 2*hidden_size\n",
    "        self.trans_W = nn.Linear(self.embedding,self.embedding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.trans_W(x)#[way,shot,vector_embedding]\n",
    "        logits = torch.zeros(x.size(0),x.size(1),1).cuda()                                                \n",
    "        for i in range(self.n_iter):\n",
    "            probs = F.softmax(logits,dim=1)\n",
    "            #print(probs.size())\n",
    "            probs_mul_x = torch.mul(x, probs.expand_as(x))\n",
    "            y = self.squash(probs_mul_x.sum(dim=1,keepdim=True))##[way,1,vector_embedding]\n",
    "            if i!= self.n_iter-1:\n",
    "                delta_logits = (y * x).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        return y  \n",
    "        \n",
    "    def squash(self, tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Relation_score(nn.Module):\n",
    "    def __init__(self,k, batch, in_channel, out_channel):\n",
    "        super(Relation_score,self).__init__()\n",
    "        self.neural_tensor_net = nn.Bilinear(in_channel,out_channel,k)\n",
    "        self.mlp = (nn.Linear(k,1))\n",
    "    def forward(self,query,classes):\n",
    "        relation_vector = self.neural_tensor_net(query,classes)\n",
    "        final_score = F.sigmoid(self.mlp(relation_vector))\n",
    "        #print(relation_vector.size())       \n",
    "        return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_2_2array(x):\n",
    "    sq_idx = x\n",
    "    \n",
    "    sq_ways_text = None #把所有classes的text集合成一个二维数组\n",
    "    for i in sq_idx:\n",
    "        if sq_ways_text==None:\n",
    "            sq_ways_text = sq_idx[i]['text_content'] \n",
    "        else:\n",
    "            sq_ways_text.extend(sq_idx[i]['text_content'])\n",
    "            \n",
    "    sq_ways_text_lens = None\n",
    "    for i in sq_idx:\n",
    "        if sq_ways_text_lens==None:\n",
    "            sq_ways_text_lens = sq_idx[i]['text_len'] \n",
    "        else:\n",
    "            sq_ways_text_lens.extend(sq_idx[i]['text_len'])\n",
    "    #print(len(sq_ways_text))\n",
    "\n",
    "    sq_ways_text = torch.tensor(sq_ways_text)\n",
    "    sq_ways_text_lens = torch.tensor(sq_ways_text_lens)\n",
    "    #print(sq_ways_text.size())\n",
    "    return sq_ways_text,sq_ways_text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sq(intent2text,intent_texts_len,way,shot,query):\n",
    "    \n",
    "    train_classes = list(intent2text.keys())\n",
    "    sample_classes = []\n",
    "    sample_classes = np.random.permutation(train_classes)[:way]\n",
    "    #print(sample_classes)\n",
    "    support_idx, query_idx = {},{}\n",
    "\n",
    "    for class_i in sample_classes: \n",
    "        sample_set = []\n",
    "        sample_set = intent2text[str(class_i)]#二维数组.intent对应的所有text\n",
    "        text_content,text_lens ,sample_s_nums = [],[],[]\n",
    "        \n",
    "        #sample_s_nums找出intent2text某一个intent（sample_set）下面，数组选哪几个位置的text，位置集合\n",
    "        sample_s_nums = np.random.permutation(len(sample_set))[:shot] \n",
    "        text_content = [sample_set[item] for item in sample_s_nums]\n",
    "        text_lens = [intent_texts_len[str(class_i)][num] for num in sample_s_nums]\n",
    "        support_idx[str(class_i)] = {'text_content':text_content,'text_len':text_lens}\n",
    "\n",
    "        #query_set构建\n",
    "        text_content,text_lens ,sample_query_nums,sample_q_nums = [],[],[],[]\n",
    "        sample_query_nums = [i for i in range(len(sample_set)) if i not in sample_s_nums]#集合减小\n",
    "        sample_q_nums = np.random.permutation(sample_query_nums)[:query]\n",
    "        text_content = [sample_set[item] for item in sample_q_nums]\n",
    "        text_lens = [intent_texts_len[str(class_i)][num] for num in sample_q_nums]\n",
    "        query_idx[str(class_i)] = {'text_content':text_content,'text_len':text_lens}\n",
    "    return support_idx, query_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fewshot(nn.Module):\n",
    "    def __init__(self, num_words, embed_size, hidden_size, emb_weight, way, shot, query, \n",
    "                 dynamic_iter,k, d_a, **kwargs):\n",
    "        super(Fewshot, self).__init__()\n",
    "        self.input_size = num_words\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_weights = emb_weight\n",
    "        self.way = way\n",
    "        self.shot = shot\n",
    "        self.query = query\n",
    "        self.k = k\n",
    "        self.encoder = Encoder(self.input_size,self.embed_size,self.hidden_size,weight=self.emb_weights,n_layers=1)\n",
    "        self.att = Attn(self.hidden_size, d_a)\n",
    "        self.dynamic_route = Dynamic_route(dynamic_iter,self.hidden_size)\n",
    "        self.relation_score = Relation_score(k, way*query, 2*self.hidden_size, 2*self.hidden_size)\n",
    "        \n",
    "    def forward(self, support_text,support_text_len,query_text,query_text_len):\n",
    "        output,hidden,_ = self.encoder(support_text,support_text_len)\n",
    "        weighted_rep = self.att(output,output.size(1))\n",
    "        C_K = weighted_rep.view(self.way, self.shot, self.hidden_size * 2)\n",
    "        Classes_vectors = self.dynamic_route(C_K)\n",
    "        \n",
    "        output,hidden,_ = self.encoder(query_text,query_text_len)\n",
    "        query_vectors = self.att(output,output.size(1))\n",
    "        ex = Classes_vectors\n",
    "        ex1 = (ex.repeat(1,50,1))\n",
    "        loss = torch.tensor(0.0).cuda()\n",
    "        for i in range(self.way):\n",
    "            groud_truth = [0.0 for i in range(self.way * self.query)]\n",
    "            groud_truth[self.query * i : self.query * (i+1)] = [1.0 for i in range(self.query)]\n",
    "            groud_truth = torch.tensor(groud_truth).cuda()\n",
    "            \n",
    "            score_i = self.relation_score(query_vectors,ex1[i]).squeeze()#type,tensor\n",
    "            loss += ((score_i - groud_truth)**2).sum()\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "import json\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import statsmodels.api as sm \n",
    "import random \n",
    "import jieba\n",
    "\n",
    "with open('xinwang_train.json') as f:\n",
    "    load_json = json.load(f)\n",
    "    data = load_json['rasa_nlu_data']['common_examples']\n",
    "for i in range(len(data)):\n",
    "    tep = data[i]['intent']\n",
    "    data[i]['intent'] = data[i]['text']\n",
    "    data[i]['text'] = tep\n",
    "#删除重复项\n",
    "res_data = []\n",
    "for item in data:\n",
    "    if not item in res_data:\n",
    "        res_data.append(item)\n",
    "intentlist = []\n",
    "for item in res_data:\n",
    "    if not item['intent'] in intentlist:\n",
    "        intentlist.append(item['intent'])\n",
    "textlist = [0 for i in range(len(intentlist))]\n",
    "for i in range(len(res_data)):\n",
    "    if res_data[i]['intent'] in intentlist:\n",
    "        textlist[intentlist.index(res_data[i]['intent'])] += 1 \n",
    "        #每一个intent下面有多少个同义句\n",
    "del_data = []\n",
    "for item in res_data:\n",
    "    if (textlist[intentlist.index(item['intent'])] >4):\n",
    "        del_data.append(item)\n",
    "    else:\n",
    "        textlist[intentlist.index(item['intent'])]  = 0\n",
    "print(len(del_data))\n",
    "data = del_data\n",
    "#把同义句低于5个的intent包含部分删除，intent包含的是所有的intent集合，text包含的是intent集合对应项的同义句的个数，\n",
    "import pickle\n",
    "intent2id = {}\n",
    "for item in data:\n",
    "    if not item['intent'] in intent2id:\n",
    "        intent2id[item['intent']] = len(intent2id)\n",
    "print(len(intent2id))\n",
    "with open(\"intent2id\",'wb') as f:\n",
    "        pickle.dump(intent2id,f)\n",
    "word2id={}\n",
    "word2id['PAD'] = 0\n",
    "word2id['UNK'] = 1\n",
    "for item in data:\n",
    "    seg_text = jieba.cut(item['text'],cut_all=False)\n",
    "    seg_text = (list(seg_text))   \n",
    "    for w in seg_text:\n",
    "        if w not in word2id:\n",
    "            word2id[w] = len(word2id)        \n",
    "    item['text']=seg_text#使用结巴分词已经划分好\n",
    "with open(\"word2id\",'wb') as f:\n",
    "        pickle.dump(word2id,f) \n",
    " #创建word2id\n",
    "data1 = data\n",
    "for item in data1:\n",
    "    for w in item['text']:\n",
    "        if w not in word2id:\n",
    "            w = 'UNK'\n",
    "for i in range(2):\n",
    "    print(data1[i])\n",
    "with open('intent2id','rb') as f:\n",
    "    intent2id = pickle.load(f)\n",
    "with open('word2id','rb') as f:\n",
    "    word2id = pickle.load(f)   \n",
    "for item in data1:\n",
    "    #print(intent2id[item['intent']])\n",
    "    item['intent'] = intent2id[item['intent']]#转化成数字label\n",
    "    item['text'] = [word2id[w] for w in item['text']]\n",
    "random.shuffle(data1)\n",
    "train_data = data1[:7300]\n",
    "test_data = data1[7300:]\n",
    "#可用的有len_data，便于lstm进行切分。data1。word2id()，intent2id\n",
    "#embedding_weight = load_pretrained_word2vec(word2id)\n",
    "#def load_pretrained_word2vec(word2id):\n",
    "#做一个intent的同义句集合(intent_same记录同义句数量)，集合i位置代表intent_i的同义句有多少个，。筛选intent同义句大于15个的data条数，\n",
    "intent_same = [0 for i in range(355)]\n",
    "for item in train_data:\n",
    "    intent_same[item['intent']] += 1\n",
    "#print(intent_same)\n",
    "#train_data1筛选出intent同义句>15\n",
    "train_data1 = []\n",
    "for item in train_data:\n",
    "    if intent_same[item['intent']]>15:\n",
    "        train_data1.append(item)\n",
    "intent2text = {}\n",
    "intent_texts_len = {}\n",
    "for item in (train_data1):\n",
    "    i = item['intent']\n",
    "    j = item['text']\n",
    "    if str(i) in intent2text:\n",
    "        intent2text[str(i)].append(j)\n",
    "        intent_texts_len[str(i)].append(len(j))\n",
    "    else:\n",
    "        intent2text[str(i)] = [j]\n",
    "        intent_texts_len[str(i)] = [len(j)]\n",
    "#检查text的长度分布,截断长度\n",
    "text_len = [0 for i in range(100)]\n",
    "for item in data1:\n",
    "    if(len(item['text']))<100:\n",
    "        text_len[len(item['text'])]+=1\n",
    "#print(text_len)\n",
    "max_len = 20\n",
    "for item in data1:\n",
    "    length = len(item['text'])\n",
    "    if length<20:\n",
    "        item['text'].extend([0] * (max_len - length))\n",
    "    else:\n",
    "        item['text'] = item['text'][:20]\n",
    "#在每个episode，挑选5个class，每个class中选5个作为example，10个作为query，\n",
    "#可用的有train_data1,intent2text(dict，intent作为key（char），text是一个二维数组),intent_texts_len\n",
    "#print(intent_texts_len)\n",
    "eposide_iter = 1000\n",
    "way = 5\n",
    "shot = 5\n",
    "query = 10\n",
    "train_classes = list(intent2text.keys())\n",
    "#print(train_classes)\n",
    "#构建sample_classes,support_idx（value是text的二维数组）,query_idx\n",
    "import numpy\n",
    "import torch\n",
    "train_classes = list(intent2text.keys())\n",
    "#for i in range(eposide_iter):\n",
    "sample_classes = np.random.permutation(train_classes)[:way]\n",
    "#print(sample_classes)\n",
    "support_idx, query_idx = {},{}\n",
    "i = 0\n",
    "for class_i in sample_classes:    \n",
    "    sample_set = intent2text[str(class_i)]#二维数组.intent对应的所有text\n",
    "    #sample_s_nums找出intent2text某一个intent（sample_set）下面，数组选哪几个位置的text，位置集合\n",
    "    sample_s_nums = np.random.permutation(len(intent2text[str(class_i)]))[:shot] \n",
    "    text_content = [sample_set[item] for item in sample_s_nums]\n",
    "    text_lens = [intent_texts_len[str(class_i)][num] for num in sample_s_nums]\n",
    "    support_idx[str(class_i)] = {'text_content':text_content,'text_len':text_lens}\n",
    "    #support_idx[str(class_i)] = [sample_set[item] for item in sample_s_nums]\n",
    "    \n",
    "    #query_set构建\n",
    "    sample_query_nums = [i for i in range(len(intent2text[str(class_i)])) if i not in sample_s_nums]#集合减小\n",
    "    sample_q_nums = np.random.permutation(sample_query_nums)[:query]\n",
    "    #query_idx[str(class_i)] = [sample_set[item] for item in sample_q_nums]\n",
    "    text_content = [sample_set[item] for item in sample_q_nums]\n",
    "    text_lens = [intent_texts_len[str(class_i)][num] for num in sample_q_nums]\n",
    "    query_idx[str(class_i)] = {'text_content':text_content,'text_len':text_lens}\n",
    "support_ways_text = None #把所有classes的text集合成一个二维数组\n",
    "for i in support_idx:\n",
    "    if support_ways_text==None:\n",
    "        support_ways_text = support_idx[i]['text_content'] \n",
    "    else:\n",
    "        support_ways_text.extend(support_idx[i]['text_content'])\n",
    "support_ways_text_lens = None\n",
    "for i in support_idx:\n",
    "    if support_ways_text_lens==None:\n",
    "        support_ways_text_lens = support_idx[i]['text_len'] \n",
    "    else:\n",
    "        support_ways_text_lens.extend(support_idx[i]['text_len'])\n",
    "print(len(support_ways_text))\n",
    "\n",
    "support_ways_text = torch.tensor(support_ways_text)\n",
    "support_ways_text_lens = torch.tensor(support_ways_text_lens)\n",
    "print(support_ways_text.size())\n",
    "#emb_weights = load_pretrained_word2vec(300)\n",
    "#with open(\"emb_weights\",'wb') as f:\n",
    " #       pickle.dump(emb_weights,f) \n",
    "with open('emb_weights','rb') as f:\n",
    "    emb_weights = pickle.load(f)\n",
    "import Ipynb_importer\n",
    "#import Untitled1\n",
    "from Untitled1 import Encoder, load_pretrained_word2vec\n",
    "import pickle\n",
    "with open('word2id','rb') as f:\n",
    "    word2id = pickle.load(f)  #2889个词\n",
    "encoder = Encoder(len(word2id),embed_size=300,hidden_size=128,n_layers=1,weight=emb_weights)\n",
    "output,hidden,_ = encoder(support_ways_text.t(),support_ways_text_lens)\n",
    "print(output.size(),hidden.size())\n",
    "#output是(max_length, batch_size, hidden_size)（先前向后反向）\n",
    "#hidden是(n_layers x num_directions, batch_size, hidden_size)\n",
    " #print(support_ways_text_lens)\n",
    "output_length = output.size(0)\n",
    "output_batch = output.size(1)\n",
    "hidden_size = 128\n",
    "d_a = 64\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size,output_batch,d_a):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = output_batch\n",
    "        self.d_a = d_a\n",
    "        self.W2_weights = nn.Parameter(torch.Tensor(1, self.d_a))\n",
    "        torch.nn.init.xavier_uniform(self.W2_weights.data)\n",
    "\n",
    "        self.W1 = (nn.Linear(self.hidden_size*2,self.d_a))\n",
    "        \n",
    "    def forward(self,encoder_outputs):\n",
    "        output = encoder_outputs\n",
    "        #encoder_outputs--[length,batch,hidden*2]      \n",
    "        W1 = torch.tanh(self.W1(output)) #[length,batch,da]\n",
    "        W1 = W1.permute(1,0,2)#[batch,length,da]\n",
    "        score = torch.bmm(W1,\n",
    "                            self.W2_weights  # (1, da)\n",
    "                            .permute(1, 0)  # (da, 1)\n",
    "                            .unsqueeze(0)  # (1, da, 1)\n",
    "                            .repeat(self.batch_size, 1, 1)\n",
    "                            # (batch_size, da, 1)\n",
    "                            )\n",
    "        score = F.softmax(F.relu(score.squeeze()))\n",
    "        weighted = torch.mul(output.permute(1,0,2), score.unsqueeze(-1).expand_as(output.permute(1,0,2)))\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "        return representations\n",
    "print(type(output),output.size())\n",
    "att = Attn(hidden_size, output_batch, d_a)\n",
    "print(type(att))\n",
    "#from Untitled1 import Attn\n",
    "att = Attn(hidden_size, output_batch, d_a)\n",
    "weighted_rep = att(output)\n",
    "print(weighted_rep.size())\n",
    "C_K = weighted_rep.view(way,shot,hidden_size*2)\n",
    "print(C_K.size())\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Dynamic_route(nn.Module):\n",
    "    def __init__(self,n_iter,embedding):\n",
    "        super(Dynamic_route,self).__init__()\n",
    "        self.n_iter = n_iter\n",
    "        self.embedding = embedding\n",
    "        self.trans_W = nn.Linear(self.embedding,self.embedding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.trans_W(x)#[way,shot,vector_embedding]\n",
    "        logits = torch.zeros(x.size(0),x.size(1),1)#.cuda()                                                \n",
    "        for i in range(self.n_iter):\n",
    "            probs = F.softmax(logits,dim=1)\n",
    "            print(probs.size())\n",
    "            probs_mul_x = torch.mul(x, probs.expand_as(x))\n",
    "            y = self.squash(probs_mul_x.sum(dim=1,keepdim=True))##[way,1,vector_embedding]\n",
    "            if i!= self.n_iter-1:\n",
    "                delta_logits = (y * x).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        return y  \n",
    "        \n",
    "    def squash(self, tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)\n",
    "dynamic_iter = 3\n",
    "#from Untitled1 import Dynamic_route\n",
    "dynamic_route = Dynamic_route(dynamic_iter,C_K.size(2))\n",
    "Classes_vectors = dynamic_route(C_K)\n",
    "print(Classes_vectors.size())\n",
    "#query_idx每一项是intent:{'text_content':[二维数组]，‘text_len’:[一维数组]}\n",
    "def dict_2_2array(x):\n",
    "    sq_idx = x\n",
    "    sq_ways_text = None #把所有classes的text集合成一个二维数组\n",
    "    for i in sq_idx:\n",
    "        if sq_ways_text==None:\n",
    "            sq_ways_text = sq_idx[i]['text_content'] \n",
    "        else:\n",
    "            sq_ways_text.extend(sq_idx[i]['text_content'])\n",
    "    sq_ways_text_lens = None\n",
    "    for i in sq_idx:\n",
    "        if sq_ways_text_lens==None:\n",
    "            sq_ways_text_lens = sq_idx[i]['text_len'] \n",
    "        else:\n",
    "            sq_ways_text_lens.extend(sq_idx[i]['text_len'])\n",
    "    print(len(sq_ways_text))\n",
    "\n",
    "    sq_ways_text = torch.tensor(sq_ways_text)\n",
    "    sq_ways_text_lens = torch.tensor(sq_ways_text_lens)\n",
    "    print(sq_ways_text.size())\n",
    "    return sq_ways_text,sq_ways_text_lens\n",
    "query_ways_text,query_ways_text_lens = dict_2_2array(query_idx)\n",
    "output,hidden,_ = encoder(query_ways_text.t(),query_ways_text_lens)\n",
    "#print(output.size(),hidden.size())\n",
    "output_batch = output.size(1)\n",
    "att = Attn(hidden_size, output_batch, d_a)\n",
    "query_vectors = att(output)\n",
    "print(query_vectors.size())\n",
    "print(query_vectors.size())\n",
    "print(Classes_vectors.size())\n",
    "ex = Classes_vectors\n",
    "ex1 = (ex.repeat(1,50,1))\n",
    "k = 100\n",
    "query_batch = query_vectors.size(0)\n",
    "print(query_batch)\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Relation_score(nn.Module):\n",
    "    def __init__(self,k, batch, in_channel, out_channel):\n",
    "        super(Relation_score,self).__init__()\n",
    "        self.neural_tensor_net = nn.Bilinear(in_channel,out_channel,k)\n",
    "        self.mlp = (nn.Linear(k,1))\n",
    "    def forward(self,query,classes):\n",
    "        relation_vector = self.neural_tensor_net(query,classes)\n",
    "        final_score = F.sigmoid(self.mlp(relation_vector))\n",
    "        #print(relation_vector.size())       \n",
    "        return final_score\n",
    "print(query_vectors.size(),ex1.size())\n",
    "relation_score = Relation_score(100,query_batch,query_vectors.size(1),query_vectors.size(1))\n",
    "loss = torch.tensor(0.0)\n",
    "for i in range(way):\n",
    "    groud_truth = [0.0 for i in range(way*query)]\n",
    "    groud_truth[query*i:query*(i+1)] = [1.0 for i in range(query)]\n",
    "    groud_truth = torch.tensor(groud_truth)\n",
    "    score_i = relation_score(query_vectors,ex1[i])#type,tensor\n",
    "    \n",
    "    loss += (score_i - groud_truth)**2\n",
    "print(loss)\n",
    "loss = torch.tensor(0.0)\n",
    "for i in range(way):\n",
    "    groud_truth = [0.0 for i in range(way*query)]\n",
    "    groud_truth[query*i:query*(i+1)] = [1.0 for i in range(query)]\n",
    "    groud_truth = torch.tensor(groud_truth)\n",
    "    score_i = relation_score(query_vectors,ex1[i])#type,tensor\n",
    "    print(score_i.size(),groud_truth.size(),(score_i - groud_truth)**2.size())\n",
    "    loss += (score_i - groud_truth)**2\n",
    "print(loss)\n",
    "loss = torch.tensor(0.0)\n",
    "for i in range(way):\n",
    "    groud_truth = [0.0 for i in range(way*query)]\n",
    "    groud_truth[query*i:query*(i+1)] = [1.0 for i in range(query)]\n",
    "    groud_truth = torch.tensor(groud_truth)\n",
    "    score_i = relation_score(query_vectors,ex1[i])#type,tensor\n",
    "    print(score_i.size(),groud_truth.size(),((score_i - groud_truth)**2).size())\n",
    "    loss += (score_i - groud_truth)**2\n",
    "print(loss)\n",
    "loss = torch.tensor(0.0)\n",
    "for i in range(way):\n",
    "    groud_truth = [0.0 for i in range(way*query)]\n",
    "    groud_truth[query*i:query*(i+1)] = [1.0 for i in range(query)]\n",
    "    groud_truth = torch.tensor(groud_truth)\n",
    "    score_i = relation_score(query_vectors,ex1[i]).squeeze()#type,tensor\n",
    "    print(score_i.size(),groud_truth.size(),((score_i - groud_truth)**2).size())\n",
    "    loss += (score_i - groud_truth)**2\n",
    "print(loss)\n",
    "loss = torch.tensor(0.0)\n",
    "for i in range(way):\n",
    "    groud_truth = [0.0 for i in range(way*query)]\n",
    "    groud_truth[query*i:query*(i+1)] = [1.0 for i in range(query)]\n",
    "    groud_truth = torch.tensor(groud_truth)\n",
    "    score_i = relation_score(query_vectors,ex1[i]).squeeze()#type,tensor\n",
    "    loss += ((score_i - groud_truth)**2).sum()\n",
    "print(loss)\n",
    "#在每个episode，挑选5个class，每个class中选5个作为example，10个作为query，\n",
    "#可用的有train_data1,intent2text(dict，intent作为key（char），text是一个二维数组),intent_texts_len\n",
    "#print(intent_texts_len)\n",
    "history\n",
    "history\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ft] *",
   "language": "python",
   "name": "conda-env-ft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
