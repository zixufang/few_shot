{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch            \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_word2vec(emb_size):\n",
    "    with open('word2id','rb') as f:\n",
    "        word2id = pickle.load(f)  #2889个\n",
    "    f.close()\n",
    "    weights = np.zeros([len(word2id),emb_size], dtype=np.float32)\n",
    "    \n",
    "    word2vec = {}\n",
    "    lines_num = 0\n",
    "    with open('financial_bigram-char',errors='ignore') as f:\n",
    "        first_line = True\n",
    "        for line in f:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                dim = int(line.rstrip().split()[1])\n",
    "                continue\n",
    "            lines_num += 1\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            word2vec[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "\n",
    "    for word_i in word2id:\n",
    "        if word_i in word2vec:\n",
    "            weights[word2id[word_i]] = word2vec[word_i]\n",
    "        else:\n",
    "            weights[word2id[word_i]] = np.random.uniform(-0.1,0.1)\n",
    "    weights = torch.from_numpy(weights)\n",
    "    return weights\n",
    "#load_pretrained_word2vec(300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_gru(gru):\n",
    "    gru.reset_parameters()\n",
    "    for _, hh, _, _ in gru.all_weights:\n",
    "        for i in range(0, hh.size(0), gru.hidden_size):\n",
    "            nn.init.orthogonal_(hh[i:i + gru.hidden_size], gain=1)\n",
    "\n",
    "\n",
    "\n",
    "#实现model的三个环节\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, n_layers, weight):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_layers = n_layers\n",
    "        #self.dropout = dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, bidirectional=True)\n",
    "        init_gru(self.gru)\n",
    "\n",
    "    def forward(self, input_seqs, input_lens, hidden=None):\n",
    "        \"\"\"\n",
    "        forward procedure. No need for inputs to be sorted\n",
    "        :param input_seqs: Variable of [T,B]\n",
    "        :param hidden:\n",
    "        :param input_lens: *numpy array* of len for each input sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = input_seqs.size(1)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        embedded = embedded.transpose(0, 1)  # [B,T,E]\n",
    "        \n",
    "        sort_idx = np.argsort(-input_lens)\n",
    "        #DELETE = cuda_(torch\n",
    "        unsort_idx = (torch.LongTensor(np.argsort(sort_idx))).cuda()\n",
    "        input_lens = input_lens[sort_idx]\n",
    "        #DELETE = cuda_(torch\n",
    "        sort_idx = (torch.LongTensor(sort_idx)).cuda()\n",
    "        embedded = embedded[sort_idx].transpose(0, 1)  # [T,B,E]\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lens)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        #print(outputs.size())\n",
    "        outputs = outputs.transpose(0, 1)[unsort_idx].transpose(0, 1).contiguous()\n",
    "        hidden = hidden.transpose(0, 1)[unsort_idx].transpose(0, 1).contiguous()\n",
    "        return outputs, hidden, embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size,d_a):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_a = d_a\n",
    "        self.W2_weights = nn.Parameter(torch.Tensor(1, self.d_a))\n",
    "        torch.nn.init.xavier_uniform(self.W2_weights.data)\n",
    "\n",
    "        self.W1 = (nn.Linear(self.hidden_size*2,self.d_a))\n",
    "        \n",
    "    def forward(self,encoder_outputs,output_batch):\n",
    "        output = encoder_outputs\n",
    "        #encoder_outputs--[length,batch,hidden*2]      \n",
    "        W1 = torch.tanh(self.W1(output)) #[length,batch,da]\n",
    "        W1 = W1.permute(1,0,2)#[batch,length,da]\n",
    "        score = torch.bmm(W1,\n",
    "                            self.W2_weights  # (1, da)\n",
    "                            .permute(1, 0)  # (da, 1)\n",
    "                            .unsqueeze(0)  # (1, da, 1)\n",
    "                            .repeat(output_batch, 1, 1)\n",
    "                            # (batch_size, da, 1)\n",
    "                            )\n",
    "        score = F.softmax(F.relu(score.squeeze()))\n",
    "        weighted = torch.mul(output.permute(1,0,2), score.unsqueeze(-1).expand_as(output.permute(1,0,2)))\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "        return representations\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Dynamic_route(nn.Module):\n",
    "    def __init__(self,n_iter,hidden_size):\n",
    "        super(Dynamic_route,self).__init__()\n",
    "        self.n_iter = n_iter\n",
    "        self.embedding = 2*hidden_size\n",
    "        self.trans_W = nn.Linear(self.embedding,self.embedding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.trans_W(x)#[way,shot,vector_embedding]\n",
    "        logits = torch.zeros(x.size(0),x.size(1),1).cuda()                                                \n",
    "        for i in range(self.n_iter):\n",
    "            probs = F.softmax(logits,dim=1)\n",
    "            #print(probs.size())\n",
    "            probs_mul_x = torch.mul(x, probs.expand_as(x))\n",
    "            y = self.squash(probs_mul_x.sum(dim=1,keepdim=True))##[way,1,vector_embedding]\n",
    "            if i!= self.n_iter-1:\n",
    "                delta_logits = (y * x).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        return y  \n",
    "        \n",
    "    def squash(self, tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Relation_score(nn.Module):\n",
    "    def __init__(self,k, batch, in_channel, out_channel):\n",
    "        super(Relation_score,self).__init__()\n",
    "        self.neural_tensor_net = nn.Bilinear(in_channel,out_channel,k)\n",
    "        self.mlp = (nn.Linear(k,1))\n",
    "    def forward(self,query,classes):\n",
    "        relation_vector = self.neural_tensor_net(query,classes)\n",
    "        final_score = F.sigmoid(self.mlp(relation_vector))\n",
    "        #print(relation_vector.size())       \n",
    "        return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_2_2array(x):\n",
    "    sq_idx = x\n",
    "    \n",
    "    sq_ways_text = None #把所有classes的text集合成一个二维数组\n",
    "    for i in sq_idx:\n",
    "        if sq_ways_text==None:\n",
    "            sq_ways_text = sq_idx[i]['text_content'] \n",
    "        else:\n",
    "            sq_ways_text.extend(sq_idx[i]['text_content'])\n",
    "            \n",
    "    sq_ways_text_lens = None\n",
    "    for i in sq_idx:\n",
    "        if sq_ways_text_lens==None:\n",
    "            sq_ways_text_lens = sq_idx[i]['text_len'] \n",
    "        else:\n",
    "            sq_ways_text_lens.extend(sq_idx[i]['text_len'])\n",
    "    #print(len(sq_ways_text))\n",
    "\n",
    "    sq_ways_text = torch.tensor(sq_ways_text)\n",
    "    sq_ways_text_lens = torch.tensor(sq_ways_text_lens)\n",
    "    #print(sq_ways_text.size())\n",
    "    return sq_ways_text,sq_ways_text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sq(intent2text,intent_texts_len,way,shot,query):\n",
    "    \n",
    "    train_classes = list(intent2text.keys())\n",
    "    sample_classes = []\n",
    "    sample_classes = np.random.permutation(train_classes)[:way]\n",
    "    #print(sample_classes)\n",
    "    support_idx, query_idx = {},{}\n",
    "\n",
    "    for class_i in sample_classes: \n",
    "        sample_set = []\n",
    "        sample_set = intent2text[str(class_i)]#二维数组.intent对应的所有text\n",
    "        text_content,text_lens ,sample_s_nums = [],[],[]\n",
    "        \n",
    "        #sample_s_nums找出intent2text某一个intent（sample_set）下面，数组选哪几个位置的text，位置集合\n",
    "        sample_s_nums = np.random.permutation(len(sample_set))[:shot] \n",
    "        text_content = [sample_set[item] for item in sample_s_nums]\n",
    "        text_lens = [intent_texts_len[str(class_i)][num] for num in sample_s_nums]\n",
    "        support_idx[str(class_i)] = {'text_content':text_content,'text_len':text_lens}\n",
    "\n",
    "        #query_set构建\n",
    "        text_content,text_lens ,sample_query_nums,sample_q_nums = [],[],[],[]\n",
    "        sample_query_nums = [i for i in range(len(sample_set)) if i not in sample_s_nums]#集合减小\n",
    "        sample_q_nums = np.random.permutation(sample_query_nums)[:query]\n",
    "        text_content = [sample_set[item] for item in sample_q_nums]\n",
    "        text_lens = [intent_texts_len[str(class_i)][num] for num in sample_q_nums]\n",
    "        query_idx[str(class_i)] = {'text_content':text_content,'text_len':text_lens}\n",
    "    return support_idx, query_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fewshot(nn.Module):\n",
    "    def __init__(self, num_words, embed_size, hidden_size, emb_weight, way, shot, query, \n",
    "                 dynamic_iter,k, d_a, **kwargs):\n",
    "        super(Fewshot, self).__init__()\n",
    "        self.input_size = num_words\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_weights = emb_weight\n",
    "        self.way = way\n",
    "        self.shot = shot\n",
    "        self.query = query\n",
    "        self.k = k\n",
    "        self.encoder = Encoder(self.input_size,self.embed_size,self.hidden_size,weight=self.emb_weights,n_layers=1)\n",
    "        self.att = Attn(self.hidden_size, d_a)\n",
    "        self.dynamic_route = Dynamic_route(dynamic_iter,self.hidden_size)\n",
    "        self.relation_score = Relation_score(k, way*query, 2*self.hidden_size, 2*self.hidden_size)\n",
    "        \n",
    "    def forward(self, support_text,support_text_len,query_text,query_text_len):\n",
    "        output,hidden,_ = self.encoder(support_text,support_text_len)\n",
    "        weighted_rep = self.att(output,output.size(1))\n",
    "        C_K = weighted_rep.view(self.way, self.shot, self.hidden_size * 2)\n",
    "        Classes_vectors = self.dynamic_route(C_K)\n",
    "        \n",
    "        output,hidden,_ = self.encoder(query_text,query_text_len)\n",
    "        query_vectors = self.att(output,output.size(1))\n",
    "        ex = Classes_vectors\n",
    "        ex1 = (ex.repeat(1,50,1))\n",
    "        loss = torch.tensor(0.0).cuda()\n",
    "        for i in range(self.way):\n",
    "            groud_truth = [0.0 for i in range(self.way * self.query)]\n",
    "            groud_truth[self.query * i : self.query * (i+1)] = [1.0 for i in range(self.query)]\n",
    "            groud_truth = torch.tensor(groud_truth).cuda()\n",
    "            \n",
    "            score_i = self.relation_score(query_vectors,ex1[i]).squeeze()#type,tensor\n",
    "            loss += ((score_i - groud_truth)**2).sum()\n",
    "        return loss\n",
    "        "
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ft] *",
   "language": "python",
   "name": "conda-env-ft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
